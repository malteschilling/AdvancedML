<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta name="generator" content="pandoc">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">     <meta name="author" content="Malte Schilling, Neuroinformatics Group, Bielefeld University">       <title>05 Gaussian Process &amp; Bayesian Models</title>
    <style type="text/css">
      code {
        white-space: pre;
      }
    </style>
        
    <link rel="stylesheet" href="../support/css/handout.css">
    <script src="../support/js/handout.js"></script>

    <!-- MathJax config -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
      menuSettings: { zoom: "Double-Click" },
      TeX: {
          Macros: {
            R: "{\\mathrm{{I}\\kern-.15em{R}}}",
            laplace: "{\\Delta}",
            grad: "{\\nabla}",
            T: "^{\\mathsf{T}}",

            norm: ['\\left\\Vert #1 \\right\\Vert', 1],
            iprod: ['\\left\\langle #1 \\right\\rangle', 1],
            vec: ['\\boldsymbol{\\mathbf{#1}}', 1],
            mat: ['\\boldsymbol{\\mathbf{#1}}', 1],
            set: ['\\mathcal{#1}', 1],
            func: ['\\mathrm{#1}', 1],
            trans: ['{#1}\\mkern-1mu^{\\mathsf{T}}', 1],
            matrix: ['\\begin{bmatrix} #1 \\end{bmatrix}', 1],
            vector: ['\\begin{pmatrix} #1 \\end{pmatrix}', 1],
            of: ['\\mkern{-2mu}\\left( #1 \\right\)', 1],
            diff: ['\\frac{\\mathrm{d}{#1}}{\\mathrm{d}{#2}}', 2],
            pdiff: ['\\frac{\\partial {#1}}{\\partial {#2}}', 2],

            vc: ['\\mathbf{#1}', 1],
            abs: ['\\lvert#1\\rvert', 1],
            norm: ['\\lVert#1\\rVert', 1],
            det: ['\\lvert#1\\rvert', 1],
            qt: ['\\hat{\\vc {#1}}', 1],
            mt: ['\\boldsymbol{#1}', 1],
            pt: ['\\boldsymbol{#1}', 1],
            textcolor: ['\\color{#1}', 1]
          }
      }
      });
    </script>
    <script src="../support/vendor/mathjax/MathJax.js?config=TeX-AMS_SVG"></script>

    <script>
      var socket = new WebSocket("ws://" + location.host + "/reload");
      socket.onmessage = function () {
        window.location.reload(true);
      };
    </script>
            <link rel="stylesheet" href="../mschilling.css">
      </head>

  <body>
        <div class="container decker-handout">
            <header>
        <div class="page-header">
          <div class="row">
            <div class="col-lg-12">
              <h1 class="title">05 Gaussian Process &amp; Bayesian Models</h1>
                            <p class="lead subtitle">Advanced Machine Learning</p>
                          </div>
          </div>
        </div>
      </header>
            <div class="row">
        <div class="col-lg-12">
          
<!-- body must not be indented or code blocks render badly -->
<!-- The following line must be left aligned! -->
<hr />
<h1 class="section" data-background-color="#2CA02C">Probabilities and Bayesian Reasoning</h1>
<hr />
<h1>Gaussian (normal) distribution</h1>
<p>Is characterized by mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma\)</span>. The probability distribution is given as</p>
<p><span class="math display">\[
p(X = x) = \mathcal{N} (x | \mu, \sigma^2) = \frac{1}{\sqrt{2\pi \sigma^2}} e^{-\frac{1}{2\sigma^2}(x-\mu)^2}
\]</span></p>
<p>The multivariate Gaussian for <span class="math inline">\(D\)</span> dimensions is given as</p>
<p><span class="math display">\[
\mathcal{N} (\vec{x} | \vec{\mu}, \Sigma) = \frac{1}{(2\pi)^{D/2} (det\ \Sigma)^{1/2}} exp\ (-\frac{1}{2} (\vec{x}-\vec{\mu})^T\Sigma^{-1}(\vec{x} - \vec{\mu}) )
\]</span></p>
<p>For <a href="https://distill.pub/2019/visual-exploration-gaussian-processes/#Multivariate">Visual Exploration of Covariance and GP</a></p>
<hr />
<h1>Bayes’ rule</h1>
<p>… tells us how to invert conditional probabilities:</p>
<p><span class="math display">\[\begin{align*}

p(A,B) &amp;= p(A|B)p(B) = p(B|A) p(A) \\
\Rightarrow p(B|A) &amp;= \frac{p(A|B) p(B)}{p(A)}
\end{align*}\]</span></p>
<p>Here,</p>
<ul>
<li><span class="math inline">\(p(B)\)</span> is the <em>a priory probability</em>, or the prior,</li>
<li><span class="math inline">\(p(A|B)\)</span> is the <em>likelihood of <span class="math inline">\(B\)</span> for a fixed <span class="math inline">\(A\)</span></em>,</li>
<li>and <span class="math inline">\(p(B|A)\)</span> is the <em>a posteriori probability</em> of <span class="math inline">\(B\)</span> given <span class="math inline">\(A\)</span>.</li>
</ul>
<hr />
<h1 class="section" data-background-color="#2CA02C">Gaussian Process – Parametric View</h1>
<hr />
<h1>Bayesian Inference</h1>
<p>Our goal is to establish inferences between inputs and targets. This is the conditional distribution of the targets given the input.</p>
<p>Our training set <span class="math inline">\(\mathcal{D}\)</span> consists of <span class="math inline">\(n\)</span> observations: <span class="math display">\[ \mathcal{D} = \{ (\vec{x}_i, y_i) | i = 1,...,n \}
\]</span></p>
<p>which we can collect in the design matrix.</p>
<div class="biblio">
<p><span class="citation" data-cites="rasmussen2006">(Rasmussen and Williams 2006)</span></p>
</div>
<hr />
<h1>A prior on parameters</h1>
<p>In a parametric model <span class="math inline">\(\mathcal{M}\)</span>, the model is defined by the structure and the parameters:</p>
<p><span class="math display">\[ f_w(\vec{x}) = \sum_{m=0}^{M} w_m \phi_m(\vec{x})\]</span></p>
<p>We can define a prior <span class="math inline">\(p(\vec{w} | \mathcal{M})\)</span> for the parameters of the model – this determines the functions the model can generate.</p>
<ul>
<li>First, we are selecting a structure.</li>
<li>Secondly, we are selecting a probability distribution for the parameters.</li>
</ul>
<hr />
<h1>Bayesian Analysis of Linear Regression</h1>
<p>We do regression on a function <span class="math inline">\(t(\vec{x}) = \vec{x}^T \vec{w}\)</span> with added Gaussian noise.</p>
<p>This leads to observation <span class="math display">\[ y = f(\vec{x}) + \varepsilon, \varepsilon \sim \mathcal{N}(\vec{0}, \sigma^2_n) \]</span></p>
<div class="box fragment">
<h2></h2>
<p>We can calculate the likelihood of the data (due to i.i.d.):</p>
<p><span class="math display">\[\begin{align*}

p(\vec{y}| \vec{X}, \vec{w})

\end{align*}\]</span></p>
</div>
<div class="box fragment">
<h2></h2>
<p>A prior on the parameters is required and we use a zero mean Gaussian with covariance matrix <span class="math inline">\(\Sigma_p\)</span>:</p>
<p><span class="math display">\[ \vec{w} \sim \mathcal{N}(\vec{0}, \Sigma_p) 
\]</span></p>
</div>
<hr />
<h1>Inference in Bayesian linear model</h1>
<p>We are looking for the posterior distribution over the weights which we get through Bayes’ rule:</p>
<p><span class="math display">\[
\text{posterior} = \frac{\text{likelihood } \times \text{ prior}}{\text{marginal likelihood}},\  p(\vec{w} | \vec{y}, \ X) = \frac{p(\vec{y}|X, w) p(\vec{w})}{p(\vec{y}| X)}
\]</span></p>
<hr />
<h1>Parametric View</h1>
<p><span class="math display">\[
f(\vec{x}) = \vec{x}^T \vec{w}, \ y = f(\vec{x}) + \varepsilon, 
\varepsilon \sim \mathcal{N} ( 0, \sigma_n^2)
\]</span></p>
<p>Reminder Gaussian probability distribution: <span class="math display">\[ g(x) = \frac{1}{\sqrt{2 \pi}\sigma} e^{-\frac{1}{2\sigma^2}(x-\mu)^2}, \mathcal{N} ( \mu, \sigma^2) \]</span></p>
<p>Likelihood: <span class="math display">\[\begin{align*}
p (\vec{y} | \mathbf{X}, \vec{w} ) &amp;= \prod_{i=1}^n p( y_i | \vec{x}_i, \vec{w}) = \prod_{i=1}^n \frac{1}{\sqrt{2 \pi}\sigma_n} e^{- \frac{(y_i - \vec{x}^T \vec{w})^2}{2\sigma_n^2}} \\
&amp;= \frac{1}{(2 \pi\sigma_n^2)^{n/2}} e^{- \frac{1}{2\sigma_n^2} |\vec{y} - \vec{x}^T \vec{w}|^2} = \mathcal{N} ( \mathbf{X}^T\vec{w}, \sigma_n^2 \mathbf{I})
\end{align*}\]</span></p>
<hr />
<h1 data-layout="columns">Setting the prior</h1>
<div class="multi-column-row multi-column-row-3">
<div class="grow-1 column column-1">
<div class="box left">
<h2 class="left"></h2>
<p>Use a zero mean Gaussian as prior on parameters:</p>
<p><span class="math display">\[
\vec{w} \sim \mathcal{N} ( 0, \Sigma_p)
\]</span></p>
<p><span class="math display">\[\begin{align*}

\text{posterior} &amp;= \frac{\text{likelihood} \times \text{prior}}{\text{marginal likelihood}}, \\ 
p(\vec{w}| \vec{y}, \mathbf{X}) &amp;= \frac{p(\vec{y}|\mathbf{X}, \vec{w}) p(\vec{w})}{p(\vec{y} | \mathbf{X})}
\end{align*}\]</span></p>
</div>
</div>
<div class="grow-1 column column-3">
<div class="box right">
<h2 class="right"></h2>
<div>
<figure><img src="../data/05/rasmussen_2_1_a_prior_w.svg" style="height:auto;width:480px;" alt="Contours of the prior distribution (1 and 2 standard deviation equi-probability lines) for f(x) = w_1 + w_2 x" title="fig:"><figcaption>Contours of the prior distribution (1 and 2 standard deviation equi-probability lines) for <span class="math inline">\(f(x) = w_1 + w_2 x\)</span></figcaption></figure>
</div>
</div>
</div>
</div>
<hr />
<h1>Deriving the posterior</h1>
<p>Importantly, the marginal likelihoodis independent of the weights and acts as a normalizing constant which does not affect the search for the best weights.</p>
<p><span class="math display">\[
p(\vec{y} | \mathbf{X}) = \int p(\vec{y} | \mathbf{X}, \vec{w}) p(\vec{w}) d\vec{w}
\]</span></p>
<p><span class="math display">\[\begin{align*} 
p(\vec{w}| \vec{y}, \mathbf{X}) &amp;\varpropto e^{- \frac{1}{2\sigma_n^2}(\vec{y} - \mathbf{X}^T \vec{w})^T (\vec{y} - \mathbf{X}^T \vec{w})} e^{ - \frac{1}{2}\vec{w}^T \Sigma_p^{-1}\vec{w} } \\

&amp;\varpropto e^{- \frac{1}{2}(\vec{w} - \bar{\vec{w}})^T (\frac{1}{\sigma_n^2} \mathbf{X} \mathbf{X}^T + \Sigma_p^{-1}) (\vec{w} - \bar{\vec{w}})}, \bar{\vec{w}} = \sigma_n^{-2} (\sigma_n^{-2} \mathbf{X} \mathbf{X}^T + \Sigma_p^{-1})^{-1}\mathbf{X}\vec{y}

\end{align*}\]</span></p>
<p>The form of the posterior distribution is again Gaussian (recognize the form) with mean <span class="math inline">\(\bar{\vec{w}}\)</span> and covariance matrix <span class="math inline">\(\mathbf{A}^{-1}\)</span>:</p>
<p><span class="math display">\[\begin{align*} 
p(\vec{w}| \vec{y}, \mathbf{X}) &amp;\sim \mathcal{N}( \bar{\vec{w}} = \frac{1}{\sigma_n^2}\mathbf{A}^{-1}\mathbf{X}\vec{y}, \mathbf{A}^{-1} ),\  \mathbf{A} = \sigma_n^{-2} \mathbf{X} \mathbf{X}^T + \Sigma_p^{-1}

\end{align*}\]</span></p>
<p>The mean of this posterior distribution maximizes the (a posterior = MAP) estimate of <span class="math inline">\(\vec{w}\)</span>.</p>
<hr />
<h1 data-layout="columns">Example of Bayesian linear model: Condition on data</h1>
<div class="multi-column-row multi-column-row-3">
<div class="grow-1 column column-1">
<div class="box left">
<h2 class="left"></h2>
<div>
<figure><img src="../data/05/rasmussen_2_1_b_data.svg" style="height:auto;width:400px;" alt="Three training data points." title="fig:"><figcaption>Three training data points.</figcaption></figure>
</div>
</div>
</div>
<div class="grow-1 column column-3">
<div class="box right">
<h2 class="right"></h2>
<div>
<figure><img src="../data/05/rasmussen_2_1_c_likelihood.svg" style="height:auto;width:400px;" alt="Likelihood p(\vec{y}|\mathbf{X}, \vec{w}) for assumed noise \sigma_n = 1" title="fig:"><figcaption>Likelihood <span class="math inline">\(p(\vec{y}|\mathbf{X}, \vec{w})\)</span> for assumed noise <span class="math inline">\(\sigma_n = 1\)</span></figcaption></figure>
</div>
</div>
</div>
</div>
<div class="single-column-row">
<div class="box bottom">
<h2 class="bottom"></h2>
<p>Slope is much more constrained/determined than intercept term.</p>
</div>
</div>
<hr />
<h1 data-layout="columns">Example of Bayesian linear model: Condition on data</h1>
<div class="multi-column-row multi-column-row-3">
<div class="grow-1 column column-1">
<div class="box left">
<h2 class="left"></h2>
<div>
<figure><img src="../data/05/rasmussen_2_1_c_likelihood.svg" style="height:auto;width:400px;" alt="Likelihood p(\vec{y}|\mathbf{X}, \vec{w}) for assumed noise \sigma_n = 1" title="fig:"><figcaption>Likelihood <span class="math inline">\(p(\vec{y}|\mathbf{X}, \vec{w})\)</span> for assumed noise <span class="math inline">\(\sigma_n = 1\)</span></figcaption></figure>
</div>
</div>
</div>
<div class="grow-1 column column-3">
<div class="box right">
<h2 class="right"></h2>
<div>
<figure><img src="../data/05/rasmussen_2_1_d_posterior.svg" style="height:auto;width:400px;" alt="Posterior p(\vec{w} | \mathbf{X}, \vec{y})." title="fig:"><figcaption>Posterior <span class="math inline">\(p(\vec{w} | \mathbf{X}, \vec{y})\)</span>.</figcaption></figure>
</div>
</div>
</div>
</div>
<div class="single-column-row">
<div class="box bottom">
<h2 class="bottom"></h2>
<p>In the posterior, the intercept is been pulled (by the prior on the weights) towards zero.</p>
</div>
</div>
<hr />
<h1>Predictive Distribution</h1>
<p>We are not choosing (as we would in non-Bayesian schemes, MAP) a specific weight. Instead, we work with the distribution over parameters which is a distribution over functions.</p>
<p>For prediction, we average over all possible parameters. This gives us a predictive distribution <span class="math inline">\(f_*\)</span> for a test case <span class="math inline">\(\vec{x}_*\)</span></p>
<p><span class="math display">\[\begin{align*} 
p(f_*| \vec{x}_*, \mathbf{X}, \vec{y}) &amp;= \int p(f_*| \vec{x}_*, \vec{w})
p(\vec{w}| \mathbf{X}, \vec{y}) d\vec{w} \\
&amp;= \mathcal{N} (\frac{1}{\sigma_n^2}\vec{x}_*^T \mathbf{A}^{-1}\
\mathbf{X} \vec{y}, \vec{x}_*^T \mathbf{A}^{-1}\vec{x}_*).
\end{align*}\]</span></p>
<p>This predictive distribution is again Gaussian.</p>
<hr />
<h1 data-layout="columns">Example of Bayesian linear model: Predcition</h1>
<div class="multi-column-row multi-column-row-3">
<div class="grow-1 column column-1">
<div class="box left">
<h2 class="left"></h2>
<p>Superimposed on the data is the predictive mean plus contours for two standard deviations of the (noise-free) predictive distribution <span class="math display">\[p(f_∗ | \vec{x}_∗, \mathbf{X}, \vec{y}).\]</span></p>
<p>which is a Gaussian probability distribution for every <span class="math inline">\(x_*\)</span> (see last slide): <span class="math display">\[\mathcal{N} (\frac{1}{\sigma_n^2}\vec{x}_*^T \mathbf{A}^{-1}\
\mathbf{X} \vec{y}, \vec{x}_*^T \mathbf{A}^{-1}\vec{x}_*).
\]</span></p>
</div>
</div>
<div class="grow-1 column column-3">
<div class="box right">
<h2 class="right"></h2>
<div>
<figure><img src="../data/05/rasmussen_2_1_b_data.svg" style="height:auto;width:480px;" alt="Three training data points." title="fig:"><figcaption>Three training data points.</figcaption></figure>
</div>
</div>
</div>
</div>
<hr />
<h1 data-layout="columns">Gaussian Processes – Bayesian Inference</h1>
<div class="multi-column-row multi-column-row-3">
<div class="grow-1 column column-1">
<div class="box left">
<h2 class="left">Prior</h2>
<div>
<figure><img src="../data/03/rasmussen_2_2_b_prior.svg" style="height:auto;width:540px;" alt="Three random function rollouts for a zero-mean prior." title="fig:"><figcaption>Three random function rollouts for a zero-mean prior.</figcaption></figure>
</div>
</div>
</div>
<div class="grow-1 column column-3">
<div class="box right fragment">
<h2 class="right">Posterior</h2>
<div>
<figure><img src="../data/03/rasmussen_2_2_b_posterior.svg" style="height:auto;width:540px;" alt="Three random function drawn from the posterior that includes example points." title="fig:"><figcaption>Three random function drawn from the posterior that includes example points.</figcaption></figure>
</div>
</div>
</div>
</div>
<div class="single-column-row">
<div class="box bottom">
<h2 class="bottom"></h2>
<div class="biblio">
<p>We are following <span class="citation" data-cites="rasmussen2006">(Rasmussen and Williams 2006)</span> and <span class="citation" data-cites="rasmussen2016">(Rasmussen 2016)</span>.</p>
</div>
</div>
</div>
<hr />
<h1 data-layout="columns">Gaussian Processes Overview</h1>
<div class="multi-column-row multi-column-row-3">
<div class="grow-1 column column-1">
<div class="box left">
<h2 class="left"></h2>
<ul>
<li>aware of uncertainty of the fitted GP that increases away from the training data,</li>
<li>let you incorporate expert knowledge,</li>
<li>are non-parametric,</li>
<li>need to take into account the whole training data for prediction.</li>
</ul>
</div>
</div>
<div class="grow-1 column column-3">
<div class="box right">
<h2 class="right"></h2>
<div>
<figure><img src="../data/03/sphx_glr_plot_gpr_noisy_targets_002.png" style="height:auto;width:480px;" alt="Three random function drawn from the posterior that includes example points." title="fig:"><figcaption>Three random function drawn from the posterior that includes example points.</figcaption></figure>
</div>
</div>
</div>
</div>
<div class="single-column-row">
<div class="box bottom">
<h2 class="bottom"></h2>
<p>Further reading: <span class="citation" data-cites="rasmussen2006">(Rasmussen and Williams 2006)</span>.</p>
</div>
</div>
<hr />
<h1 class="columns">Two Bayesian Perspectives on Functions</h1>
<div class="single-column-row">
<div class="box top">
<h2 class="top"></h2>
<p>Create Gaussian Distribution for each variable – distribute these through your space.</p>
<p>Informally such an infinite long vector constitutes a function.</p>
</div>
</div>
<div class="multi-column-row multi-column-row-3">
<div class="grow-1 column column-1">
<div class="box left">
<h2 class="left">Prior</h2>
<p><img src="../data/04/ritter_prior_samples.svg" style="height:auto;width:450px;"></p>
</div>
</div>
<div class="grow-1 column column-3">
<div class="box right fragment">
<h2 class="right">Posterior</h2>
<p><img src="../data/04/ritter_posterior_samples.svg" style="height:auto;width:450px;"></p>
</div>
</div>
</div>
<div class="single-column-row">
<div class="box bottom fragment">
<h2 class="bottom"></h2>
<p>A Gaussian process is a collection of random variables, any finite number of which have (consistent) Gaussian distributions.</p>
</div>
</div>
<hr />
<h1 class="sub"> </h1>
<p><iframe src="http://www.it.uu.se/edu/course/homepage/apml/GP/index.html">Browser does not support iframe.</iframe></p>
<hr />
<h1 class="section" data-background-color="#2CA02C">Gaussian Process – Distribution over Parameters</h1>
<hr />
<h1 class="columns sub">Example: A prior distribution over functions</h1>
<div class="multi-column-row multi-column-row-3">
<div class="grow-1 column column-1">
<div class="box left">
<h2 class="left"></h2>
<p>As an example,</p>
<ul>
<li>we choose a polynomical model with <span class="math inline">\(M = 17\)</span>: <span class="math inline">\(\phi_m(\vec{x}) = \vec{x}^m\)</span></li>
<li>as a prior for the parameter distribution we choose a normal distribution: <span class="math display">\[p(w_m) = \mathcal{N} (w_m | \mu, \sigma_w^2)\]</span></li>
</ul>
</div>
</div>
<div class="grow-1 column column-3">
<div class="box right">
<h2 class="right"></h2>
<p><img src="../data/04/rasmussen2016_parametric_function.svg" style="height:auto;width:540px;"></p>
</div>
</div>
</div>
<div class="single-column-row">
<div class="box bottom">
<h2 class="bottom"></h2>
<p>Shown is one example for which we sampled all the parameters from the normal distribution.</p>
</div>
</div>
<hr />
<h1>Distribution over functions</h1>
<ul>
<li>We have seen now an algorithm for building a model through selecting the model type and sample parameters.</li>
</ul>
<div class="incremental">
<ul class="incremental">
<li><p>But we are interested in predictions of the model and not the parameters as such.</p></li>
<li><p>Secondly, we want to work directly in the space of functions. This becomes possible as a distribution over parameters induces a distribution over functions <span class="math inline">\(p(\vec{f} | \mathcal{M})\)</span>.</p></li>
<li><p>This would be simpler and allow for more efficient inference.</p></li>
</ul>
</div>
<hr />
<h1 class="columns">Posterior probabilities for a function</h1>
<div class="multi-column-row multi-column-row-3">
<div class="grow-1 column column-1">
<div class="box left">
<h2 class="left"></h2>
<p>Our goal is to use our functions <span class="math inline">\(\vec{f}\)</span> to make predictions for novel inputs. But until now, we have only looked at the prior for these functions <span class="math inline">\(p(\vec{f}| \mathcal{M})\)</span>.</p>
<p>We are interested in the posterior distribution of the function – that is which is conditioned on our evidence:</p>
<p><span class="math display">\[\begin{align*}

p(\vec{f} | \vec{y}) = \frac{p(\vec{y}|\vec{f}) p(\vec{f})}{p(\vec{y})}

\end{align*}\]</span></p>
</div>
</div>
<div class="grow-1 column column-3">
<div class="box right">
<h2 class="right"></h2>
<p><img src="../data/04/rasmussen2016_sample_posterior.svg" style="height:auto;width:400px;"></p>
<p>Sample from the posterior <span class="citation" data-cites="rasmussen2016">(Rasmussen 2016)</span></p>
</div>
</div>
</div>
<div class="single-column-row">
<div class="box bottom">
<h2 class="bottom"></h2>
<div class="incremental">
<ul class="incremental">
<li>we can consider this as: when sampling from the prior, allow only sampled functions that fit the data (go through the data points)</li>
<li>closeness to the data is given through the likelihood <span class="math inline">\(p(\vec{y}|\vec{f})\)</span></li>
</ul>
</div>
</div>
</div>
<hr />
<h1>Drawback of polynomials as priors for functions</h1>
<p><img src="../data/04/rasmussen2016_polynomial_samples.svg" style="height:auto;width:800px;"></p>
<p>Shown are samples for parameters for polynomial functions of different order <span class="citation" data-cites="rasmussen2016">(Rasmussen 2016)</span>.</p>
<hr />
<h1 class="sub">Drawback of sampling over parameters</h1>
<div class="incremental">
<ul class="incremental">
<li>Distributions over parameters induce distribution over functions.</li>
<li>But sampling over parameter space and using priors over functions might not lead to good results (see example for polynomials).</li>
<li>Therefore, we want to work directly on priors and probability distributions over functions.</li>
<li>This leads to the question of how probability distribution over functions look like and how they could be specified.</li>
</ul>
</div>
<hr />
<h1> </h1>
<p><iframe src="http://www.it.uu.se/edu/course/homepage/apml/GP/index.html">Browser does not support iframe.</iframe></p>
<hr />
<h1 data-layout="columns">Gaussian Processes</h1>
<div class="multi-column-row multi-column-row-3">
<div class="grow-1 column column-1">
<div class="box left">
<h2 class="left">Prior</h2>
<div>
<figure><img src="../data/03/rasmussen_2_2_b_prior.svg" style="height:auto;width:540px;" alt="Three random function rollouts for a zero-mean prior." title="fig:"><figcaption>Three random function rollouts for a zero-mean prior.</figcaption></figure>
</div>
</div>
</div>
<div class="grow-1 column column-3">
<div class="box right fragment">
<h2 class="right">Posterior</h2>
<div>
<figure><img src="../data/03/rasmussen_2_2_b_posterior.svg" style="height:auto;width:540px;" alt="Three random function drawn from the posterior that includes example points." title="fig:"><figcaption>Three random function drawn from the posterior that includes example points.</figcaption></figure>
</div>
</div>
</div>
</div>
<div class="single-column-row">
<div class="box bottom">
<h2 class="bottom"></h2>
<div class="biblio">
<p>We are following <span class="citation" data-cites="rasmussen2006">(Rasmussen and Williams 2006)</span> and <span class="citation" data-cites="rasmussen2016">(Rasmussen 2016)</span>.</p>
</div>
</div>
</div>
<hr />
<h1 class="unnumbered biblio">References</h1>
<div id="refs" class="references" role="doc-bibliography">
<div id="ref-rasmussen2016">
<p>Rasmussen, Carl Edward. 2016. “Probabilistic Machine Learning.” Lecture Notes, University of Cambridge.</p>
</div>
<div id="ref-rasmussen2006">
<p>Rasmussen, CE., and CKI. Williams. 2006. <em>Gaussian Processes for Machine Learning</em>. Adaptive Computation and Machine Learning. Cambridge, MA, USA: Biologische Kybernetik; Max-Planck-Gesellschaft; MIT Press.</p>
</div>
</div>
<!-- The previous line must be left aligned! -->

                              <hr>
          <address>
                        <p class="author"> Malte Schilling, Neuroinformatics Group, Bielefeld University</p>
                      </address>
        </div>
      </div>
    </div>
  </body>


  </html>